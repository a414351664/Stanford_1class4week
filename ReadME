layer_dims = (n_x, 20, 14, 5, n_y)
0.01的学习率　　迭代2000次的效果
2层99％，　72％
# ５层的神经网络随机初始化时×１效果99％，　76％过拟合　　　　×２效果65％，　34％
# ７层的神经网络随机初始化时×１效果99％，　74％过拟合　　　　×２效果65％，　34％
对猫进行分类，不同层的神经网络之间的效果对比

0.0075的学习率　　迭代2000次的效果 
２层×１效果98％，　72％		×２效果98％，　72％
# ５层的神经网络随机初始化时×１效果99％，　56％过拟合　　　　×２效果99％，　60％
# ７层的神经网络随机初始化时×１效果99％，　74％过拟合　　　　×２效果65％，　34％

early_stoping 
0.0075的学习率　　迭代1500次的效果
２层×１效果95％，　74％		×２效果95％，　74％


layer_dims = (n_x, 20, 7, 5, n_y)
0.0075的学习率　　迭代2000次的效果 
# ５层的神经网络随机初始化时×１效果95％，　82％过拟合　　　　×２
0.01的学习率　　　　　　　　　　　99％　　84％		　　×２　　82％　　78％

乘以１的效果比较好总体上看来。可能的原因是神经网络的层数还是不够大，增加层数需要在数据集足够的情况下，而该实例的数据集又较小，所以以后考察。

从逻辑回归到2层神经网络，测试集从70->72％，再到5层的神经网络，->84%,效果还是不错的。但对于同一层的神经元的数量不同又会造成不同的效果，有待实验考究原因。
